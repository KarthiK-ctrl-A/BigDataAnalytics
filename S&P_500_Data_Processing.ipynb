{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarthiK-ctrl-A/BigDataAnalytics/blob/main/S%26P_500_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial code to fetch the Stock data from github and make it into a single CSV file"
      ],
      "metadata": {
        "id": "1lkiYYSMyciu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import pyfinancialdata as fd\n",
        "# import pandas as pd\n",
        "\n",
        "# # Fetch data from 2010 to 2018\n",
        "# data_2010_2018 = pd.DataFrame()\n",
        "# for year in range(2010, 2019):\n",
        "#   try:\n",
        "#     data = fd.get(provider='histdata', instrument='SPXUSD', year=year)\n",
        "#     data_2010_2018 = pd.concat([data_2010_2018, data])\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error fetching data for {year}: {e}\")\n",
        "\n",
        "# # Ensure the 'Date' column is of datetime type and set as index\n",
        "# if 'Date' in data_2010_2018.columns:\n",
        "#     data_2010_2018['Date'] = pd.to_datetime(data_2010_2018['Date'])\n",
        "#     data_2010_2018 = data_2010_2018.set_index('Date')\n",
        "\n",
        "\n",
        "# # Save the data to a CSV file, including the date index\n",
        "# data_2010_2018.to_csv('/UMKC_Assignments/spxusd_2010_2018.csv')\n",
        "\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# # Load the data from the CSV file\n",
        "# data_2010_2018 = pd.read_csv('/UMKC_Assignments/spxusd_2010_2018.csv', index_col='date', parse_dates=True)\n",
        "\n",
        "# # Convert the index to datetime objects if it's not already\n",
        "# # data_2010_2018.index = pd.to_datetime(data_2010_2018.index)\n",
        "\n",
        "# # Filter the data for the specified time range\n",
        "# start_time = pd.to_datetime('09:00').time()\n",
        "# end_time = pd.to_datetime('15:29').time()\n",
        "\n",
        "# filtered_data = data_2010_2018.between_time(start_time, end_time)\n",
        "\n",
        "\n",
        "\n",
        "# # Display or further process the filtered data\n",
        "# filtered_data.to_csv('/UMKC_Assignments/spxusd_9am_329pm.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3WUPGwZQPzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using, already fetched data and then performing the data preprocessing steps"
      ],
      "metadata": {
        "id": "2wR_2L-rylz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "filtered_data = pd.read_csv('/content/spxusd_9am_329pm.csv')\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming 'filtered_data' DataFrame is already loaded and available\n",
        "\n",
        "# Select columns for normalization\n",
        "columns_to_normalize = ['open', 'high', 'low', 'close']\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the selected columns\n",
        "scaler.fit(filtered_data[columns_to_normalize])\n",
        "\n",
        "# Transform the selected columns\n",
        "normalized_data = scaler.transform(filtered_data[columns_to_normalize])\n",
        "\n",
        "# Create a new DataFrame with normalized columns\n",
        "normalized_df = pd.DataFrame(normalized_data, columns=[f'{col}_normalized' for col in columns_to_normalize], index=filtered_data.index)\n",
        "\n",
        "# Concatenate the normalized columns with the original DataFrame\n",
        "filtered_data = pd.concat([filtered_data, normalized_df], axis=1)\n",
        "\n",
        "# Now 'filtered_data' contains both original and normalized columns\n",
        "print(filtered_data.head())\n",
        "\n",
        "\n",
        "# # Merge the two dataframes based on their index\n",
        "# merged_df = pd.merge(filtered_data, normalized_df, left_index=True, right_index=True)\n",
        "\n",
        "# print(merged_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opgCYWbKxAIk",
        "outputId": "62ad5899-8741-4a1a-9119-18224388a061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  date     open     high      low    close  open_normalized  \\\n",
            "0  2010-11-15 09:00:00  1200.50  1200.50  1200.25  1200.50         0.070154   \n",
            "1  2010-11-15 09:01:00  1200.75  1200.75  1200.50  1200.75         0.070288   \n",
            "2  2010-11-15 09:02:00  1201.00  1201.00  1201.00  1201.00         0.070421   \n",
            "3  2010-11-15 09:04:00  1200.75  1200.75  1200.50  1200.75         0.070288   \n",
            "4  2010-11-15 09:06:00  1200.50  1200.50  1200.25  1200.25         0.070154   \n",
            "\n",
            "   high_normalized  low_normalized  close_normalized  \n",
            "0         0.069783        0.070421          0.070173  \n",
            "1         0.069916        0.070554          0.070306  \n",
            "2         0.070049        0.070820          0.070439  \n",
            "3         0.069916        0.070554          0.070306  \n",
            "4         0.069783        0.070421          0.070040  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, LayerNormalization, MultiHeadAttention, Dropout\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# Assuming 'filtered_data' DataFrame is already loaded and contains 'close_normalized'\n",
        "\n",
        "# Prepare the data for time series forecasting\n",
        "data = filtered_data['close_normalized'].values.reshape(-1, 1)\n",
        "look_back = 60  # Example lookback period\n",
        "\n",
        "X, y = [], []\n",
        "for i in range(look_back, len(data)):\n",
        "    X.append(data[i - look_back:i, 0])\n",
        "    y.append(data[i, 0])\n",
        "X, y = np.array(X), np.array(y)\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Normalize the data using MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
        "X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
        "\n",
        "# Define the Transformer model\n",
        "class TimeSeriesTransformer(Model):\n",
        "    def __init__(self, input_shape, d_model=64, num_heads=4, num_layers=3, output_size=1):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.input_layer = Input(shape=input_shape)\n",
        "\n",
        "        # Multi-head attention layer\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.norm1 = LayerNormalization()\n",
        "        self.dropout1 = Dropout(0.1)\n",
        "\n",
        "        # Feed-forward layer\n",
        "        self.dense1 = Dense(128, activation='relu')\n",
        "        self.dense2 = Dense(output_size)\n",
        "\n",
        "        # Positional encoding is not necessary here since we are using a simple approach, but can be added if needed\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Attention block\n",
        "        attn_output = self.attention(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(attn_output + inputs)\n",
        "\n",
        "        # Feed-forward block\n",
        "        output = self.dense1(out1)\n",
        "        output = self.dense2(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Build the model\n",
        "model = TimeSeriesTransformer(input_shape=(X_train.shape[1], 1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)  # Adjust epochs and batch_size as needed\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions to get actual prices\n",
        "predictions = predictions.reshape(-1, 1)  # Reshape predictions to 2D\n",
        "predictions = scaler.inverse_transform(predictions)  # Inverse transform\n",
        "\n",
        "# Inverse transform the y_test values\n",
        "y_test = y_test.reshape(-1, 1)  # Reshape y_test to 2D\n",
        "y_test = scaler.inverse_transform(y_test)  # Inverse transform\n",
        "\n",
        "# Prepare the last 'look_back' data points from the training set\n",
        "last_data = data[-look_back:].reshape(1, look_back, 1)  # Reshaping to (1, look_back, 1)\n",
        "\n",
        "# Predict the next 30 minutes\n",
        "forecast = []\n",
        "for i in range(30):\n",
        "    # Predict the next price\n",
        "    next_price = model.predict(last_data)\n",
        "\n",
        "    # Extract the scalar value from the prediction\n",
        "    next_price_scalar = next_price[0, 0]  # Extracting the scalar value\n",
        "\n",
        "    # Append the predicted value to the forecast list\n",
        "    forecast.append(next_price_scalar)  # Store the predicted value\n",
        "\n",
        "    # Update the input sequence by appending the predicted price\n",
        "    last_data = np.roll(last_data, -1, axis=1)  # Shift the sequence to the left\n",
        "    last_data[0, -1, 0] = next_price_scalar  # Replace the last value with the prediction\n",
        "\n",
        "# Inverse transform the forecasted prices to get the actual predicted prices\n",
        "forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))\n",
        "\n",
        "print(\"Forecasted close prices for the next 30 minutes:\")\n",
        "print(forecast)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYq3N8UewxQF",
        "outputId": "e9bed6d8-bdd2-464b-e15d-5deecbe420d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3ms/step - loss: 0.0423\n",
            "Epoch 2/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 3/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 4/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 5/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 6/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 7/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 8/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 9/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 10/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 11/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 12/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 13/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0363\n",
            "Epoch 14/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 15/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 16/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 17/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 18/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 19/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 20/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3ms/step - loss: 0.0365\n",
            "\u001b[1m4653/4653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-84804ea6d7d7>:95: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  last_data[0, -1, 0] = next_price_scalar  # Replace the last value with the prediction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Forecasted close prices for the next 30 minutes:\n",
            "[[0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "\n",
        "# Function to fetch historical data and normalize it\n",
        "def fetch_and_normalize_data(ticker, period='1d', interval='1m', look_back=60):\n",
        "    # Fetch historical minute-level data\n",
        "    data = yf.download(ticker, period=period, interval=interval)\n",
        "\n",
        "    # Select the features (Open, High, Low, Close)\n",
        "    features = data[['Open', 'High', 'Low', 'Close']]\n",
        "\n",
        "    # Normalize using the MinMaxScaler\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    # Use the last 'look_back' number of data points for prediction\n",
        "    last_data = features_scaled[-look_back:]\n",
        "\n",
        "    return last_data, scaler\n",
        "\n",
        "# Function to check if the market is open\n",
        "def is_market_open():\n",
        "    cst_timezone = pytz.timezone('America/New_York')\n",
        "    current_time = datetime.now(cst_timezone)\n",
        "    weekday = current_time.weekday()  # Monday=0, Sunday=6\n",
        "\n",
        "    # Market is open from 9:30 AM to 4:00 PM EST, Monday to Friday\n",
        "    if weekday < 5:  # Weekday (Mon-Fri)\n",
        "        market_open = current_time.replace(hour=9, minute=30, second=0, microsecond=0)\n",
        "        market_close = current_time.replace(hour=16, minute=0, second=0, microsecond=0)\n",
        "\n",
        "        # Check if current time is within market hours\n",
        "        if market_open <= current_time <= market_close:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to predict the next close price and use it for subsequent predictions\n",
        "def predict_next_close(ticker, look_back=60, forecast_period=30):\n",
        "    # Fetch and normalize the data\n",
        "    last_data, scaler = fetch_and_normalize_data(ticker, look_back=look_back)\n",
        "\n",
        "    # Prepare the last data point for the model\n",
        "    last_data_input = last_data.reshape(1, look_back, 4)  # Reshape for LSTM input (1 sample, 60 time steps, 4 features)\n",
        "\n",
        "    forecast = []  # Array to hold predictions\n",
        "    timestamps = []  # Array to hold timestamps\n",
        "\n",
        "    # Get current time and set the initial timestamp\n",
        "    cst_timezone = pytz.timezone('America/New_York')\n",
        "    current_time = datetime.now(cst_timezone)\n",
        "\n",
        "    # Check if market is open\n",
        "    market_open = is_market_open()\n",
        "\n",
        "    # Predict for the next 'forecast_period' minutes\n",
        "    for _ in range(forecast_period):\n",
        "        # Simulate the prediction by using the last close value and add a small random fluctuation\n",
        "        next_close_scaled = last_data[-1, 3]  # Get the scaled last close value\n",
        "\n",
        "        # Inverse transform to get the original scale\n",
        "        next_close = scaler.inverse_transform(np.array([[0, 0, 0, next_close_scaled]]))[:, 3]\n",
        "\n",
        "        # Add fluctuation only if market is open\n",
        "        if market_open:\n",
        "            fluctuation = np.random.normal(0, 0.001)  # Small random fluctuation (mean=0, std=0.001)\n",
        "            next_close += fluctuation * next_close\n",
        "\n",
        "        # Append the predicted value and timestamp to their respective lists\n",
        "        forecast.append(next_close[0])\n",
        "        timestamps.append(current_time.strftime('%Y-%m-%d %H:%M:%S'))  # Format timestamp\n",
        "\n",
        "        # Update the current time by adding 1 minute\n",
        "        current_time += timedelta(minutes=1)\n",
        "\n",
        "        # Update the last_data to include the new prediction (use predicted values continuously)\n",
        "        next_close_scaled_for_update = scaler.transform(np.array([[next_close[0], next_close[0], next_close[0], next_close[0]]]))[0, 3]  # Get the normalized value of predicted close\n",
        "        last_data = np.roll(last_data, -1, axis=0)  # Shift the data\n",
        "        last_data[-1, 3] = next_close_scaled_for_update  # Use the newly predicted value for the next iteration\n",
        "\n",
        "    return timestamps, np.array(forecast)\n",
        "\n",
        "# Example usage with the SPX (S&P 500 Index)\n",
        "ticker = '^GSPC'\n",
        "\n",
        "# Get the predicted next 30 minutes of close prices with timestamps\n",
        "timestamps, predicted_prices = predict_next_close(ticker, look_back=60, forecast_period=30)\n",
        "\n",
        "# Print the predicted 30 minutes of close prices with timestamps\n",
        "for timestamp, price in zip(timestamps, predicted_prices):\n",
        "    print(f\"{timestamp} - Predicted Close: {price:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt65toh-FKSn",
        "outputId": "9c4c2d1c-5535-498e-e436-ed4854a93d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-21 16:54:33 - Predicted Close: 6013.46\n",
            "2025-02-21 16:55:33 - Predicted Close: 6013.46\n",
            "2025-02-21 16:56:33 - Predicted Close: 6013.46\n",
            "2025-02-21 16:57:33 - Predicted Close: 6013.46\n",
            "2025-02-21 16:58:33 - Predicted Close: 6013.46\n",
            "2025-02-21 16:59:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:00:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:01:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:02:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:03:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:04:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:05:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:06:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:07:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:08:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:09:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:10:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:11:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:12:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:13:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:14:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:15:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:16:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:17:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:18:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:19:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:20:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:21:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:22:33 - Predicted Close: 6013.46\n",
            "2025-02-21 17:23:33 - Predicted Close: 6013.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from flask import Flask, jsonify, render_template_string\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Initialize the Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Function to fetch historical data and normalize it\n",
        "def fetch_and_normalize_data(ticker, period='1d', interval='1m', look_back=60):\n",
        "    data = yf.download(ticker, period=period, interval=interval)\n",
        "    features = data[['Open', 'High', 'Low', 'Close']]\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    last_data = features_scaled[-look_back:]\n",
        "    return last_data, scaler\n",
        "\n",
        "# Function to check if the market is open (Eastern Time)\n",
        "def is_market_open():\n",
        "    # Convert to Eastern Time (ET)\n",
        "    et_timezone = pytz.timezone('America/New_York')\n",
        "    current_time = datetime.now(et_timezone)\n",
        "    weekday = current_time.weekday()  # Monday=0, Sunday=6\n",
        "\n",
        "    # Market is open from 9:30 AM to 4:00 PM ET, Monday to Friday\n",
        "    if weekday < 5:  # Weekday (Mon-Fri)\n",
        "        market_open = current_time.replace(hour=9, minute=30, second=0, microsecond=0)\n",
        "        market_close = current_time.replace(hour=16, minute=0, second=0, microsecond=0)\n",
        "\n",
        "        # Check if current time is within market hours\n",
        "        if market_open <= current_time <= market_close:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to predict the next close price and use it for subsequent predictions\n",
        "def predict_next_close(ticker, look_back=60, forecast_period=30):\n",
        "    last_data, scaler = fetch_and_normalize_data(ticker, look_back=look_back)\n",
        "    forecast = []\n",
        "    timestamps = []\n",
        "    et_timezone = pytz.timezone('America/New_York')\n",
        "    current_time = datetime.now(et_timezone)\n",
        "\n",
        "    # Only predict if the market is open\n",
        "    if not is_market_open():\n",
        "        return {\"message\": \"Market is closed. Predictions can only be made during market hours (9:30 AM - 4:00 PM ET).\"}\n",
        "\n",
        "    for _ in range(forecast_period):\n",
        "        next_close_scaled = last_data[-1, 3]\n",
        "        next_close = scaler.inverse_transform(np.array([[0, 0, 0, next_close_scaled]]))[:, 3]\n",
        "        fluctuation = np.random.normal(0, 0.001)\n",
        "        next_close += fluctuation * next_close\n",
        "        forecast.append(next_close[0])\n",
        "        timestamps.append(current_time.strftime('%Y-%m-%d %H:%M:%S'))\n",
        "        current_time += timedelta(minutes=1)\n",
        "        next_close_scaled_for_update = scaler.transform(np.array([[next_close[0], next_close[0], next_close[0], next_close[0]]]))[0, 3]\n",
        "        last_data = np.roll(last_data, -1, axis=0)\n",
        "        last_data[-1, 3] = next_close_scaled_for_update\n",
        "    return {\"timestamps\": timestamps, \"predicted_prices\": forecast}\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    # HTML template for Chart.js\n",
        "    html_template = '''\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "        <title>Stock Price Predictions</title>\n",
        "        <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Predicted Stock Prices</h1>\n",
        "        <canvas id=\"predictionChart\" width=\"800\" height=\"400\"></canvas>\n",
        "\n",
        "        <script>\n",
        "            const ctx = document.getElementById('predictionChart').getContext('2d');\n",
        "            let chart;\n",
        "\n",
        "            // Fetch the predicted data from the Flask API\n",
        "            async function fetchPredictions() {\n",
        "                const response = await fetch('/predict');\n",
        "                const data = await response.json();\n",
        "                return data;\n",
        "            }\n",
        "\n",
        "            // Function to create and update the chart\n",
        "            async function createChart() {\n",
        "                const data = await fetchPredictions();\n",
        "                const timestamps = data.timestamps;\n",
        "                const predictedPrices = data.predicted_prices;\n",
        "\n",
        "                if (chart) {\n",
        "                    chart.destroy(); // Destroy the old chart before creating a new one\n",
        "                }\n",
        "\n",
        "                chart = new Chart(ctx, {\n",
        "                    type: 'line',\n",
        "                    data: {\n",
        "                        labels: timestamps,  // X-axis labels (timestamps)\n",
        "                        datasets: [{\n",
        "                            label: 'Predicted Close Price',\n",
        "                            data: predictedPrices,  // Y-axis data (predicted prices)\n",
        "                            borderColor: 'rgba(75, 192, 192, 1)',\n",
        "                            backgroundColor: 'rgba(75, 192, 192, 0.2)',\n",
        "                            borderWidth: 1\n",
        "                        }]\n",
        "                    },\n",
        "                    options: {\n",
        "                        scales: {\n",
        "                            x: {\n",
        "                                title: {\n",
        "                                    display: true,\n",
        "                                    text: 'Time'\n",
        "                                }\n",
        "                            },\n",
        "                            y: {\n",
        "                                title: {\n",
        "                                    display: true,\n",
        "                                    text: 'Price (USD)'\n",
        "                                }\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                });\n",
        "            }\n",
        "\n",
        "            // Call the function to create the chart when the page loads\n",
        "            createChart();\n",
        "        </script>\n",
        "    </body>\n",
        "    </html>\n",
        "    '''\n",
        "    return render_template_string(html_template)\n",
        "\n",
        "@app.route('/predict')\n",
        "def predict():\n",
        "    ticker = '^GSPC'\n",
        "    result = predict_next_close(ticker, look_back=60, forecast_period=30)\n",
        "    # Return the prediction or a message if market is closed\n",
        "    return jsonify(result)\n",
        "\n",
        "# Start the ngrok tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\" * ngrok URL: {public_url}\")\n",
        "\n",
        "# Run the Flask app\n",
        "app.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgLfGzFLOKrA",
        "outputId": "d4d200dc-1425-4686-fb56-7c444881a401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok URL: NgrokTunnel: \"https://3b74-34-83-189-209.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Feb/2025 21:57:22] \"GET / HTTP/1.1\" 200 -\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Feb/2025 21:57:22] \"GET /predict HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Feb/2025 21:57:22] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING:pyngrok.process.ngrok:t=2025-02-21T21:57:46+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5000-e2b9ae2c-6a8b-4192-9f86-5e0ace0c06e8 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-02-21T21:57:46+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-5000-e2b9ae2c-6a8b-4192-9f86-5e0ace0c06e8 err=\"failed to start tunnel: session closed\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HPy4YprITPg1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}